{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!/usr/bin/env python\n",
    "\"\"\"plot_AWS_comparison.py\n",
    "\n",
    "Script plots the point time series against AWS data to evaluate model data\n",
    "\n",
    "Author: Annette L Hirsch @ CLEX, UNSW. Sydney (Australia)\n",
    "email: a.hirsch@unsw.edu.au\n",
    "Created: Thu Jul 30 14:26:12 AEST 2020\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import netCDF4 as nc\n",
    "import sys\n",
    "import os\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import xarray as xr\n",
    "from matplotlib.collections import LineCollection\n",
    "import common_functions as cf\n",
    "import datetime as dt\n",
    "import wrf\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Period\n",
    "syear = 2017\n",
    "smon = 1\n",
    "sday = 2 \n",
    "eyear = 2017\n",
    "emon = 2\n",
    "eday = 28  # Add an extra day so that the 27th Feb data is included\n",
    "simlen = dt.datetime(eyear,emon,eday) - dt.datetime(syear,smon,sday)\n",
    "nst = (simlen.days * 24 * 6) # No. simulations days x 24 hours in a day x 6 history intervals per hour\n",
    "\n",
    "# Dates - Used for subsetting the AWS data so you pick the day before the start date and the day after the end date\n",
    "sdate = \"2017-01-01\"\n",
    "edate = \"2017-02-28\"\n",
    "\n",
    "# Data directory \n",
    "datadir='/g/data/w97/azh561/WRF/'\n",
    "ensmem = ['sydney800m','sydney800m_06H','sydney800m_12H','sydney800m_18H','sydney800m_00H']\n",
    "rlabels = ['U1','U2','U3','U4','U5']\n",
    "domain = [\"d02\",\"d02\",\"d02\",\"d02\",\"d02\"]\n",
    "#domain = [\"d01\",\"d01\",\"d01\",\"d01\",\"d01\"]\n",
    "nmem = len(ensmem)\n",
    "\n",
    "# Landsea mask\n",
    "mask_file='/g/data/w97/azh561/WRF/sydney800m_06H/geo_em.%s.nc' %(domain[0])\n",
    "f = nc.Dataset(mask_file)\n",
    "lu = f.variables['LU_INDEX'][0,:,:]\n",
    "lat2d = f.variables['XLAT_M'][0,:,:]\n",
    "lontmp = f.variables['XLONG_M'][0,:,:]\n",
    "lon2d = np.where(lontmp<0.0,lontmp+360,lontmp)\n",
    "clon = f.getncattr('CEN_LON')\n",
    "nlu = f.getncattr('NUM_LAND_CAT')\n",
    "iswater = f.getncattr('ISWATER')\n",
    "f.close()\n",
    "\n",
    "print(\"Domain %s Lat: %s to %s Lon: %s to %s\" %(domain[0],np.nanmin(lat2d),np.nanmax(lat2d),np.nanmin(lon2d),np.nanmax(lon2d)))\n",
    "\n",
    "# Figure Details\n",
    "fig_dir='%s/figures/' %(os.getcwd())\n",
    "fig_name_prefix='AWS_comparison_'\n",
    "if not os.path.exists(fig_dir):\n",
    "  os.makedirs(fig_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awsdir = '/g/data/w97/azh561/WRF/obs/AWS_1mindata_20stations'\n",
    "awsnum = ['066037','066137','066194','067105','067108','067113','061078','061366','066062','067119','068228']\n",
    "awsnm = ['Sydney Airport','Bankstown Airport','Canterbury Racecourse','Richmond RAAF','Badgerys Creek','Penrith Lakes','Williamtown RAAF','Norah Head','Sydney Observatory Hill','Horsley Park','Bellambi']\n",
    "awslat = [-33.9465,-33.9176,-33.9057,-33.6004,-33.8969,-33.7195,-32.7939,-33.2814,-33.8607,-33.851,-34.3691]\n",
    "awslon = [151.1731,150.9837,151.1134,150.7761,150.7281,150.6783,151.8364,151.5766,151.2050,150.8567,150.9291]\n",
    "naws = len(awsnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to note the bad quality observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_values(data):\n",
    "    \n",
    "    # https://stackoverflow.com/questions/19909167/how-to-find-most-frequent-string-element-in-numpy-ndarray\n",
    "    unique,pos = np.unique(data,return_inverse=True)\n",
    "    counts = np.bincount(pos)\n",
    "    maxpos = counts.argmax() # To find the positions of the max count\n",
    "    \n",
    "    if unique[maxpos] in ['Y'] and counts[maxpos] == 10:\n",
    "        qcflag = np.nan\n",
    "    else:\n",
    "        qcflag = 0.05\n",
    "        \n",
    "    return qcflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the 10 minute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_10min_avg(dataframe):\n",
    "    \n",
    "    # Variables of interest\n",
    "    otc = dataframe['tc']\n",
    "    orh = dataframe['rh']\n",
    "    odp = dataframe['dp']\n",
    "    omslp = dataframe['mslp']\n",
    "    opr = dataframe['pr']\n",
    "    owspd = dataframe['wspd']\n",
    "    owdir = dataframe['wdir']\n",
    "\n",
    "    # Quality control flags\n",
    "    otc_qc = dataframe['tc_qc']\n",
    "    orh_qc = dataframe['rh_qc']\n",
    "    odp_qc = dataframe['dp_qc']\n",
    "    omslp_qc = dataframe['mslp_qc']\n",
    "    opr_qc = dataframe['pr_qc']\n",
    "    owspd_qc = dataframe['wspd_qc']\n",
    "    owdir_qc = dataframe['wdir_qc']    \n",
    "\n",
    "    # Calculate the 10-minute averages - NOT THE MOST EFFICIENT WAY I'M SURE OF IT!\n",
    "    odata = np.empty((7,nst),dtype=np.float64)\n",
    "    oqc = np.empty((7,nst),dtype=np.float64)\n",
    "    for tt in range(nst):\n",
    "\n",
    "        # MSLP\n",
    "        odata[0,tt] = np.mean([float(omslp.iloc[(tt*10)]),float(omslp.iloc[(tt*10)+1]),float(omslp.iloc[(tt*10)+2]),\n",
    "             float(omslp.iloc[(tt*10)+3]),float(omslp.iloc[(tt*10)+4]),float(omslp.iloc[(tt*10)+5]),\n",
    "             float(omslp.iloc[(tt*10)+6]),float(omslp.iloc[(tt*10)+7]),float(omslp.iloc[(tt*10)+8]),\n",
    "             float(omslp.iloc[(tt*10)+9])])\n",
    "        # T2\n",
    "        odata[1,tt] = np.mean([float(otc.iloc[(tt*10)]),float(otc.iloc[(tt*10)+1]),float(otc.iloc[(tt*10)+2]),\n",
    "             float(otc.iloc[(tt*10)+3]),float(otc.iloc[(tt*10)+4]),float(otc.iloc[(tt*10)+5]),\n",
    "             float(otc.iloc[(tt*10)+6]),float(otc.iloc[(tt*10)+7]),float(otc.iloc[(tt*10)+8]),\n",
    "             float(otc.iloc[(tt*10)+9])])\n",
    "        # TD2\n",
    "        odata[2,tt] = np.mean([float(odp.iloc[(tt*10)]),float(odp.iloc[(tt*10)+1]),float(odp.iloc[(tt*10)+2]),\n",
    "             float(odp.iloc[(tt*10)+3]),float(odp.iloc[(tt*10)+4]),float(odp.iloc[(tt*10)+5]),\n",
    "             float(odp.iloc[(tt*10)+6]),float(odp.iloc[(tt*10)+7]),float(odp.iloc[(tt*10)+8]),\n",
    "             float(odp.iloc[(tt*10)+9])])\n",
    "        # RH2\n",
    "        odata[3,tt] = np.mean([float(orh.iloc[(tt*10)]),float(orh.iloc[(tt*10)+1]),float(orh.iloc[(tt*10)+2]),\n",
    "             float(orh.iloc[(tt*10)+3]),float(orh.iloc[(tt*10)+4]),float(orh.iloc[(tt*10)+5]),\n",
    "             float(orh.iloc[(tt*10)+6]),float(orh.iloc[(tt*10)+7]),float(orh.iloc[(tt*10)+8]),\n",
    "             float(orh.iloc[(tt*10)+9])])\n",
    "        # PR - total rather than mean\n",
    "        odata[4,tt] = np.sum([float(opr.iloc[(tt*10)]),float(opr.iloc[(tt*10)+1]),float(opr.iloc[(tt*10)+2]),\n",
    "             float(opr.iloc[(tt*10)+3]),float(opr.iloc[(tt*10)+4]),float(opr.iloc[(tt*10)+5]),\n",
    "             float(opr.iloc[(tt*10)+6]),float(opr.iloc[(tt*10)+7]),float(opr.iloc[(tt*10)+8]),\n",
    "             float(opr.iloc[(tt*10)+9])])\n",
    "        # Wind Speed\n",
    "        odata[5,tt] = np.mean([float(owspd.iloc[(tt*10)]),float(owspd.iloc[(tt*10)+1]),float(owspd.iloc[(tt*10)+2]),\n",
    "             float(owspd.iloc[(tt*10)+3]),float(owspd.iloc[(tt*10)+4]),float(owspd.iloc[(tt*10)+5]),\n",
    "             float(owspd.iloc[(tt*10)+6]),float(owspd.iloc[(tt*10)+7]),float(owspd.iloc[(tt*10)+8]),\n",
    "             float(owspd.iloc[(tt*10)+9])])\n",
    "        # Wind Dir\n",
    "        odata[6,tt] = np.mean([float(owdir.iloc[(tt*10)]),float(owdir.iloc[(tt*10)+1]),float(owdir.iloc[(tt*10)+2]),\n",
    "             float(owdir.iloc[(tt*10)+3]),float(owdir.iloc[(tt*10)+4]),float(owdir.iloc[(tt*10)+5]),\n",
    "             float(owdir.iloc[(tt*10)+6]),float(owdir.iloc[(tt*10)+7]),float(owdir.iloc[(tt*10)+8]),\n",
    "             float(owdir.iloc[(tt*10)+9])])\n",
    "\n",
    "        ### Get the instances where the data quality is bad\n",
    "        oqc[0,tt] = get_bad_values([omslp_qc.iloc[(tt*10)],omslp_qc.iloc[(tt*10)+1],omslp_qc.iloc[(tt*10)+2],\n",
    "             omslp_qc.iloc[(tt*10)+3],omslp_qc.iloc[(tt*10)+4],omslp_qc.iloc[(tt*10)+5],\n",
    "             omslp_qc.iloc[(tt*10)+6],omslp_qc.iloc[(tt*10)+7],omslp_qc.iloc[(tt*10)+8],\n",
    "             omslp_qc.iloc[(tt*10)+9]])\n",
    "        oqc[1,tt] = get_bad_values([otc_qc.iloc[(tt*10)],otc_qc.iloc[(tt*10)+1],otc_qc.iloc[(tt*10)+2],\n",
    "             otc_qc.iloc[(tt*10)+3],otc_qc.iloc[(tt*10)+4],otc_qc.iloc[(tt*10)+5],\n",
    "             otc_qc.iloc[(tt*10)+6],otc_qc.iloc[(tt*10)+7],otc_qc.iloc[(tt*10)+8],\n",
    "             otc_qc.iloc[(tt*10)+9]])\n",
    "        oqc[2,tt] = get_bad_values([odp_qc.iloc[(tt*10)],odp_qc.iloc[(tt*10)+1],odp_qc.iloc[(tt*10)+2],\n",
    "             odp_qc.iloc[(tt*10)+3],odp_qc.iloc[(tt*10)+4],odp_qc.iloc[(tt*10)+5],\n",
    "             odp_qc.iloc[(tt*10)+6],odp_qc.iloc[(tt*10)+7],odp_qc.iloc[(tt*10)+8],\n",
    "             odp_qc.iloc[(tt*10)+9]])\n",
    "        oqc[3,tt] = get_bad_values([orh_qc.iloc[(tt*10)],orh_qc.iloc[(tt*10)+1],orh_qc.iloc[(tt*10)+2],\n",
    "             orh_qc.iloc[(tt*10)+3],orh_qc.iloc[(tt*10)+4],orh_qc.iloc[(tt*10)+5],\n",
    "             orh_qc.iloc[(tt*10)+6],orh_qc.iloc[(tt*10)+7],orh_qc.iloc[(tt*10)+8],\n",
    "             orh_qc.iloc[(tt*10)+9]])\n",
    "        oqc[4,tt] = get_bad_values([opr_qc.iloc[(tt*10)],opr_qc.iloc[(tt*10)+1],opr_qc.iloc[(tt*10)+2],\n",
    "             opr_qc.iloc[(tt*10)+3],opr_qc.iloc[(tt*10)+4],opr_qc.iloc[(tt*10)+5],\n",
    "             opr_qc.iloc[(tt*10)+6],opr_qc.iloc[(tt*10)+7],opr_qc.iloc[(tt*10)+8],\n",
    "             opr_qc.iloc[(tt*10)+9]])\n",
    "        oqc[5,tt] = get_bad_values([owspd_qc.iloc[(tt*10)],owspd_qc.iloc[(tt*10)+1],owspd_qc.iloc[(tt*10)+2],\n",
    "             owspd_qc.iloc[(tt*10)+3],owspd_qc.iloc[(tt*10)+4],owspd_qc.iloc[(tt*10)+5],\n",
    "             owspd_qc.iloc[(tt*10)+6],owspd_qc.iloc[(tt*10)+7],owspd_qc.iloc[(tt*10)+8],\n",
    "             owspd_qc.iloc[(tt*10)+9]])\n",
    "        oqc[6,tt] = get_bad_values([owdir_qc.iloc[(tt*10)],owdir_qc.iloc[(tt*10)+1],owdir_qc.iloc[(tt*10)+2],\n",
    "             owdir_qc.iloc[(tt*10)+3],owdir_qc.iloc[(tt*10)+4],owdir_qc.iloc[(tt*10)+5],\n",
    "             owdir_qc.iloc[(tt*10)+6],owdir_qc.iloc[(tt*10)+7],owdir_qc.iloc[(tt*10)+8],\n",
    "             owdir_qc.iloc[(tt*10)+9]])\n",
    "\n",
    "    return odata,oqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and calculate the 10-minute averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odata = np.empty((naws,7,nst),dtype=np.float64)\n",
    "oqc = np.empty((naws,7,nst),dtype=np.float64)\n",
    "for ss in range(naws):\n",
    "    \n",
    "    # Read data\n",
    "    file = \"%s/HD01D_Data_%s_46163679534753.txt\" %(awsdir,awsnum[ss])\n",
    "    data = pd.read_csv(file)\n",
    "    # MUST USE THE UTC TIME SO THAT WRF AND AWS DATA TIMES ARE THE SAME\n",
    "    data.columns = [\"a\",\"No\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"year\",\"month\",\"day\",\"hour\",\"minute\",\n",
    "                \"pr\",\"pr_qc\",\"l\",\"tc\",\"tc_qc\",\"wbt\",\"wbt_qc\",\"dp\",\"dp_qc\",\"rh\",\"rh_qc\",\"vp\",\"vp_qc\",\n",
    "                \"svp\",\"svp_qc\",\"wspd\",\"wspd_qc\",\"wdir\",\"wdir_qc\",\"m\",\"n\",\"o\",\"p\",\"vis\",\"vis_qc\",\"mslp\",\"mslp_qc\",\"q\"]\n",
    "\n",
    "    data['date'] = pd.to_datetime(data[['year','month','day']])\n",
    "    data['time'] = pd.to_datetime(data[['year','month','day','hour','minute']])\n",
    "\n",
    "    # Clip to period of interest\n",
    "    date_filter = data.loc[(data['date'] > sdate) & (data['date'] < edate)]\n",
    "\n",
    "    # Deal with empty cells\n",
    "    date_filter = date_filter.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    # Calculate the 10 minute averages\n",
    "    odata[ss,:,:],oqc[ss,:,:] = calc_10min_avg(date_filter)\n",
    "    \n",
    "    del data,date_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Model data closest to the stations - Iterate through groups of files \n",
    "    takes 40 mins per ensemble member for the 2 months of simulation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slat = np.empty(naws,dtype=np.int)\n",
    "slon = np.empty(naws,dtype=np.int)\n",
    "for ss in range(naws):\n",
    "    \n",
    "    # Get lat/lon corresponding to the AWS site\n",
    "    # https://stackoverflow.com/questions/28006077/find-i-j-location-of-closest-long-lat-values-in-a-2d-array\n",
    "    a = abs(lat2d-awslat[ss])+abs(lon2d-awslon[ss])\n",
    "    i0,j0 = np.unravel_index(a.argmin(),a.shape)\n",
    "\n",
    "    slat[ss] = i0\n",
    "    slon[ss] = j0\n",
    "    \n",
    "    del a,i0,j0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mm in range(nmem):\n",
    "\n",
    "    # Files list\n",
    "    filelist = sorted(glob.glob('%s/%s/WRF_output/%s/wrfout_%s_2017-*' %(datadir,ensmem[mm],domain[mm],domain[mm])))\n",
    "    nfile = len(filelist)\n",
    "\n",
    "    #for ff in range(simlen.days):\n",
    "    for ff in range(int(nfile/24)):\n",
    "\n",
    "        wrffiles = [nc.Dataset(filelist[(ff*24)]),nc.Dataset(filelist[(ff*24)+1]),nc.Dataset(filelist[(ff*24)+2])\n",
    "        ,nc.Dataset(filelist[(ff*24)+3]),nc.Dataset(filelist[(ff*24)+4]),nc.Dataset(filelist[(ff*24)+5])\n",
    "        ,nc.Dataset(filelist[(ff*24)+6]),nc.Dataset(filelist[(ff*24)+7]),nc.Dataset(filelist[(ff*24)+8])\n",
    "        ,nc.Dataset(filelist[(ff*24)+9]),nc.Dataset(filelist[(ff*24)+10]),nc.Dataset(filelist[(ff*24)+11])\n",
    "        ,nc.Dataset(filelist[(ff*24)+12]),nc.Dataset(filelist[(ff*24)+13]),nc.Dataset(filelist[(ff*24)+14])\n",
    "        ,nc.Dataset(filelist[(ff*24)+15]),nc.Dataset(filelist[(ff*24)+16]),nc.Dataset(filelist[(ff*24)+17])\n",
    "        ,nc.Dataset(filelist[(ff*24)+18]),nc.Dataset(filelist[(ff*24)+19]),nc.Dataset(filelist[(ff*24)+20])\n",
    "        ,nc.Dataset(filelist[(ff*24)+21]),nc.Dataset(filelist[(ff*24)+22]),nc.Dataset(filelist[(ff*24)+23])]\n",
    "\n",
    "        # Extract the variables of interest\n",
    "        timetmp  = wrf.getvar(wrffiles,\"times\",timeidx=None,method='cat')               # Times\n",
    "        rh2tmp   = wrf.getvar(wrffiles,\"rh2\",timeidx=None,method='cat')[:,slat,slon]                         # 2m Relative Humidity\n",
    "        td2tmp   = wrf.getvar(wrffiles,\"td2\",units='degC',timeidx=None,method='cat')[:,slat,slon]            # 2m Dew Point Temperature\n",
    "        t2tmp    = wrf.getvar(wrffiles,'T2',timeidx=None,method='cat')[:,slat,slon] - 273.15                               # 2m temperature\n",
    "        psfctmp  = wrf.getvar(wrffiles,'PSFC',timeidx=None,method='cat')[:,slat,slon] /100.                                # surface pressure hPa\n",
    "        prtmp    = wrf.getvar(wrffiles,'RAINC',timeidx=None,method='cat')[:,slat,slon] + wrf.getvar(wrffiles,'RAINNC',timeidx=None,method='cat')[:,slat,slon] # total precipitation mm\n",
    "        windtmp  = wrf.getvar(wrffiles,\"wspd_wdir10\",units='km h-1',timeidx=None,method='cat')[:,:,slat,slon] # 10m wind speed and direction\n",
    "\n",
    "        # Append to arrays\n",
    "        if ff == 0:\n",
    "            ftimes = timetmp\n",
    "            rh2 = rh2tmp\n",
    "            td2 = td2tmp\n",
    "            t2 = t2tmp\n",
    "            psfc = psfctmp\n",
    "            pr = prtmp\n",
    "            wspd = windtmp[0,:,:,:]\n",
    "            wdir = windtmp[1,:,:,:]\n",
    "        else:\n",
    "            ftimes = np.append(ftimes,timetmp,axis=0)\n",
    "            rh2 = np.append(rh2,rh2tmp,axis=0)\n",
    "            td2 = np.append(td2,td2tmp,axis=0)\n",
    "            t2 = np.append(t2,t2tmp,axis=0)\n",
    "            psfc = np.append(psfc,psfctmp,axis=0)\n",
    "            pr = np.append(pr,prtmp,axis=0)\n",
    "            wspd = np.append(wspd,windtmp[0,:,:,:],axis=0)\n",
    "            wdir = np.append(wdir,windtmp[1,:,:,:],axis=0)\n",
    "\n",
    "        # Cleanup\n",
    "        del timetmp,rh2tmp,td2tmp,t2tmp,psfctmp,prtmp,windtmp\n",
    "\n",
    "    ftimes = ftimes.astype('datetime64[m]')\n",
    "\n",
    "    if mm == 0:\n",
    "        tsdata = np.empty((naws,nmem,7,len(ftimes)),dtype=np.float64) # [nmem+1,nvar,ntime]\n",
    "        times = ftimes\n",
    "\n",
    "    for ss in range(naws):\n",
    "        tsdata[ss,mm,0,:len(ftimes)] = psfc[:len(ftimes),ss,ss]\n",
    "        tsdata[ss,mm,1,:len(ftimes)] = t2[:len(ftimes),ss,ss]\n",
    "        tsdata[ss,mm,2,:len(ftimes)] = td2[:len(ftimes),ss,ss]\n",
    "        tsdata[ss,mm,3,:len(ftimes)] = rh2[:len(ftimes),ss,ss]\n",
    "        tsdata[ss,mm,4,:len(ftimes)] = 0.0 # First set all pr values to zero\n",
    "        for tt in range(len(pr)-1):\n",
    "            tsdata[ss,mm,4,tt] = pr[tt+1,ss,ss] - pr[tt,ss,ss]\n",
    "        tsdata[ss,mm,5,:len(ftimes)] = wspd[:len(ftimes),ss,ss]\n",
    "        tsdata[ss,mm,6,:len(ftimes)] = wdir[:len(ftimes),ss,ss]\n",
    "\n",
    "    # Once data read for an ensemble member - write to file - saves read time later!\n",
    "    for ss in range(naws):\n",
    "        datadump = np.vstack([tsdata[ss,mm,0,:len(ftimes)], tsdata[ss,mm,1,:len(ftimes)], tsdata[ss,mm,2,:len(ftimes)],\n",
    "         tsdata[ss,mm,3,:len(ftimes)], tsdata[ss,mm,4,:len(ftimes)], tsdata[ss,mm,5,:len(ftimes)],\n",
    "            tsdata[ss,mm,6,:len(ftimes)]])\n",
    "        np.savetxt('WRF_output_M%s_%s_AWS%s.txt' %(ensmem[mm],domain[mm],awsnum[ss]), (datadump.T), delimiter = ' ',header = \"PSFC T2 TD2 RH2 PR WSPD WDIR\", fmt = '%0.4f %0.4f %0.4f %0.4f %0.4f %0.4f %0.4f')\n",
    "        del datadump\n",
    "        \n",
    "    del psfc,t2,td2,rh2,pr,wspd,wdir,filelist,nfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the previously extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime(syear,smon,sday,0,0,0)\n",
    "end = dt.datetime(eyear,emon,eday,0,0,0)\n",
    "days = (end - start).days\n",
    "ntim = days * 24 * 60\n",
    "datelist = [start + dt.timedelta(minutes=x) for x in range(ntim+1)]\n",
    "# Get the day-month hour-minutes on 10 minute interval\n",
    "ftimes = np.asarray([datelist[x].strftime(\"%m-%d %H-%M\") for x in range(ntim+1)])[::10]\n",
    "fdates = np.asarray([datelist[x].strftime(\"%m-%d\") for x in range(ntim+1)])[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = np.empty((naws,nmem,7,len(ftimes)),dtype=np.float64)\n",
    "# Loop through the ensemble members\n",
    "for mm in range(nmem):\n",
    "\n",
    "    # Loop through the sites\n",
    "    for ss in range(naws):\n",
    "    \n",
    "        # Read data\n",
    "        data = pd.read_csv('WRFOUT_AWS_EXTRACTED_GRIDS/WRF_output_M%s_%s_AWS%s.txt' %(ensmem[mm],domain[mm],awsnum[ss]),delimiter = ' ')\n",
    "        data.columns = [\"PSFC\", \"T2\", \"TD2\", \"RH2\", \"PR\", \"WSPD\", \"WDIR\",\"#\"]\n",
    "    \n",
    "        tsdata[ss,mm,0,:] = data['PSFC'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,1,:] = data['T2'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,2,:] = data['TD2'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,3,:] = data['RH2'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,4,:] = data['PR'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,5,:] = data['WSPD'].iloc[0:len(ftimes)]\n",
    "        tsdata[ss,mm,6,:] = data['WDIR'].iloc[0:len(ftimes)]\n",
    "    \n",
    "        del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with time files of the WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spind = [i for i in range(len(times)) if times[i] in [np.datetime64('2017-02-28T00:00')]][0]\n",
    "spind = [i for i in range(len(ftimes)) if ftimes[i] in ['02-28 00-00']][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot data\n",
    "def plot_ts(time,tsdata,odata,oqc,rlabels,vlabels,mtitle,figurename,lspace):\n",
    "\n",
    "    \"\"\"This function plots time series for observations and models\"\"\"\n",
    "\n",
    "    from matplotlib.colors import BoundaryNorm\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    import string\n",
    "    \n",
    "    # Figure formatting\n",
    "    plt.rcParams['savefig.dpi']=500\n",
    "    plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "    plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "    # Define dimensions\n",
    "    nmod = tsdata.shape[0]\n",
    "    nvar = tsdata.shape[1]\n",
    "    nt = tsdata.shape[2]\n",
    "      \n",
    "    # Create figure object and subplots\n",
    "    fig, ax = plt.subplots(nvar, 1, figsize=(30.0,5.0*(nvar)), squeeze=False)\n",
    "    tarr = np.arange(0,nt)\n",
    "    \n",
    "    colors = [\"red\",\"blue\",\"green\",\"orange\",\"purple\"] \n",
    "    ypos = [0.9,0.8,0.7,0.6,0.5]\n",
    "    \n",
    "    # Iterate through variables\n",
    "    for vind in range(nvar):\n",
    "\n",
    "        # Observations\n",
    "        ax[vind,0].plot(tarr,odata[vind,:],linestyle='-',linewidth=4,color='black',label='AWS')\n",
    "\n",
    "        # Add the observation flags where the is poor data quality\n",
    "        ax2 = ax[vind,0].twinx()\n",
    "        ax2.plot(tarr,oqc[vind,:],marker='o',markersize=3,linestyle = 'None',color='DarkGrey')\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.set_yticks([],[])\n",
    "        ax2.set_xticks([],[])\n",
    "        ax2.set_xlim(tarr[0],tarr[-1])\n",
    "\n",
    "        # Models\n",
    "        for mind in range(nmod):\n",
    "            if mind == 2:\n",
    "                xdata = tarr[:len(ftimes)]\n",
    "                ydata = tsdata[mind,vind,:len(ftimes)]\n",
    "                oy = odata[vind,:len(ftimes)]\n",
    "            else:\n",
    "                xdata = tarr\n",
    "                ydata = tsdata[mind,vind,:]\n",
    "                oy = odata[vind,:]\n",
    "            ax[vind,0].plot(xdata,ydata, linewidth=2,color=colors[mind], linestyle='-', label=rlabels[mind])\n",
    "            # Add statistics\n",
    "            bias = np.round(cf.calc_bias(ydata,oy,lat2d,flag=False),2)\n",
    "            rmse = np.round(cf.calc_rmse(ydata,oy,lat2d,flag=False),2)\n",
    "            phase = np.round(cf.calc_phase(ydata,oy,lat2d,flag=False),2)\n",
    "            ax[vind,0].text(0.07,ypos[mind],'$S_{B}$=%s, $S_{R}$=%s, $S_{P}$=%s' %(bias,rmse,phase), \n",
    "                            horizontalalignment='center',verticalalignment='center',transform = ax[vind,0].transAxes,\n",
    "                            color=colors[mind], fontweight='bold', fontsize=18)\n",
    "            del bias,rmse,phase\n",
    "            del xdata,ydata,oy\n",
    "\n",
    "        # Fix Labelling\n",
    "        ax[vind,0].set_ylabel('%s' %(vlabels[vind]), fontweight = 'bold',fontsize=20)\n",
    " \n",
    "        # Amend axis limits\n",
    "        ax[vind,0].set_xlim(tarr[0]-1300,tarr[-1])\n",
    "        \n",
    "        if vind < nvar-1:\n",
    "            ax[vind,0].set_xticks([],[])\n",
    "        else:\n",
    "            ax[vind,0].set_xticks(tarr[::lspace])\n",
    "            ax[vind,0].set_xticklabels(time[::lspace],rotation=90,fontsize=18)\n",
    "\n",
    "    ax[0,0].set_title(mtitle, fontweight = 'bold',fontsize=20)\n",
    "    legend = ax[-1,0].legend(loc='upper center', bbox_to_anchor=(0.5,-0.275), ncol=6, fontsize=20)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.savefig(figurename,bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlabels = ['MSLP [hPa]','$T_{2m}$ [\\xb0 C]','$T_{dp}$ [\\xb0 C]', 'RH [%]','PR [mm]','WSPD [$km.hr^{-1}$]','WDIR [\\xb0]']\n",
    "lspace = 144 # As the wrf output is saved at a 10 minute interval\n",
    "\n",
    "for ss in range(naws):\n",
    "    figurename = 'Times_Series_Validation_%s.png' %(awsnm[ss])\n",
    "    plot_ts(fdates[:spind],tsdata[ss,:,:,:spind],odata[ss,:,:spind],oqc[ss,:,:spind],rlabels,vlabels,awsnm[ss],figurename,lspace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For d01 domain\n",
    "\n",
    "vlabels = ['MSLP [hPa]','$T_{2m}$ [\\xb0 C]','$T_{dp}$ [\\xb0 C]', 'RH [%]','PR [mm]','WSPD [$km.hr^{-1}$]','WDIR [\\xb0]']\n",
    "lspace = 144 # As the wrf output is saved at a 10 minute interval\n",
    "spind = 8200\n",
    "\n",
    "for ss in range(naws):\n",
    "    figurename = 'Times_Series_Validation_d01_%s.png' %(awsnm[ss])\n",
    "    plot_ts(ftimes[:spind],tsdata[ss,:,:,:spind],odata[ss,:,:spind],oqc[ss,:,:spind],rlabels,vlabels,awsnm[ss],figurename,lspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the statistics of the ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlabels = ['MSLP [hPa]','$T_{2m}$ [\\xb0 C]','$T_{dp}$ [\\xb0 C]', 'RH [%]','PR [mm]','WSPD [$km.hr^{-1}$]','WDIR [\\xb0]']\n",
    "for ss in range(naws):\n",
    "    print(awsnm[ss])\n",
    "    tsmean = np.nanmean(tsdata[ss,:,:,:spind],axis=0)\n",
    "    \n",
    "    for vind in range(7):\n",
    "        print(vlabels[vind])\n",
    "        bias = np.round(cf.calc_bias(tsmean[vind,:],odata[ss,vind,:spind],lat2d,flag=False),2)\n",
    "        rmse = np.round(cf.calc_rmse(tsmean[vind,:],odata[ss,vind,:spind],lat2d,flag=False),2)\n",
    "        phase = np.round(cf.calc_phase(tsmean[vind,:],odata[ss,vind,:spind],lat2d,flag=False),2) \n",
    "        \n",
    "        print([bias,rmse,phase])\n",
    "        del bias,rmse,phase\n",
    "        \n",
    "    del tsmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of the AWS file\n",
    "    1  hd\n",
    "    2  Station Number\n",
    "#### datetime format in Local time\n",
    "    3  YYYY\n",
    "    4  MM \n",
    "    5  DD\n",
    "    6  HH24\n",
    "    7  MI\n",
    "#### datetime format in Local standard time\n",
    "    8  YYYY\n",
    "    9  MM \n",
    "    10 DD\n",
    "    11 HH24\n",
    "    12 MI\n",
    "#### datetime format in Universal coordinated time\n",
    "    13 YYYY\n",
    "    14 MM \n",
    "    15 DD\n",
    "    16 HH24\n",
    "    17 MI\n",
    "#### variables\n",
    "    18 Precipitation since last (AWS) observation in mm\n",
    "    19 Quality of precipitation since last (AWS) observation value\n",
    "    20 Period over which precipitation since last (AWS) observation is measured in minutes\n",
    "    21 Air Temperature in degrees Celsius\n",
    "    22 Quality of air temperature\n",
    "    23 Wet bulb temperature in degrees Celsius\n",
    "    24 Quality of Wet bulb temperature\n",
    "    25 Dew point temperature in degrees Celsius\n",
    "    26 Quality of dew point temperature\n",
    "    27 Relative humidity in percentage %\n",
    "    28 Quality of relative humidity\n",
    "    29 Vapour pressure in hPa\n",
    "    30 Quality of vapour pressure\n",
    "    31 Saturated vapour pressure in hPa\n",
    "    32 Quality of saturated vapour pressure\n",
    "    33 Wind (1 minute) speed in km/h,\n",
    "    34 Wind (1 minute) speed quality\n",
    "    35 Wind (1 minute) direction in degrees true\n",
    "    36 Wind (1 minute) direction quality\n",
    "    37 Standard deviation of wind (1 minute) \n",
    "    38 Standard deviation of wind (1 minute) direction quality\n",
    "    39 Maximum wind gust (over 1 minute) in km/h\n",
    "    40 Maximum wind gust (over 1 minute) quality\n",
    "    41 Visibility (automatic - one minute data) in km\n",
    "    42 Quality of visibility (automatic - one minute data) \n",
    "    43 Mean sea level pressure in hPa\n",
    "    44 Quality of mean sea level pressure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
